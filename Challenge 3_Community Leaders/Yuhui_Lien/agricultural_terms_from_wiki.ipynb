{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LVrPXrE7HzDT"
      },
      "outputs": [],
      "source": [
        "#pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ak9yeY5RTx0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hLYDP5IWTWUE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_glossary_terms(url):\n",
        "    \"\"\"\n",
        "    Downloads a Wikipedia glossary page by adding a User-Agent header\n",
        "    and extracts terms found within <dt> tags.\n",
        "    \"\"\"\n",
        "    # Define a standard User-Agent header to mimic a web browser\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # 1. Download the webpage content, passing the headers\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "        print(\"Successfully fetched the page content.\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching URL: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 2. Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # 3. Find and extract all glossary terms\n",
        "    glossary_terms = []\n",
        "\n",
        "    # We look for <dt> tags inside the main content area\n",
        "    body_content = soup.find(id='bodyContent')\n",
        "\n",
        "    if body_content:\n",
        "        for dt_tag in body_content.find_all('dt'):\n",
        "            # Extract and clean the text of the term\n",
        "            term = dt_tag.get_text().strip()\n",
        "\n",
        "            if term:\n",
        "                # Basic cleaning: takes the first line and removes reference brackets\n",
        "                term_clean = term.split('\\n')[0].split('[')[0].strip()\n",
        "                glossary_terms.append(term_clean)\n",
        "\n",
        "    return glossary_terms\n",
        "\n",
        "# The URL of the agricultural glossary\n",
        "wikipedia_url = \"https://en.wikipedia.org/wiki/Glossary_of_agriculture\"\n",
        "\n",
        "# Execute the function and get the list of terms\n",
        "terms_list = extract_glossary_terms(wikipedia_url)\n",
        "\n",
        "# Print the results\n",
        "if terms_list:\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"✅ Successfully extracted {len(terms_list)} unique agricultural terms.\")\n",
        "    print(\"-\" * 40)\n",
        "    # Print the first 15 terms as an example\n",
        "    print(\"First 15 Extracted Terms:\")\n",
        "    for term in terms_list[:15]:\n",
        "        print(f\"* {term}\")\n",
        "    print(\"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZk_o-noIP8A",
        "outputId": "69a93198-82e9-40f0-8dd5-d9392acf7c2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully fetched the page content.\n",
            "----------------------------------------\n",
            "✅ Successfully extracted 856 unique agricultural terms.\n",
            "----------------------------------------\n",
            "First 15 Extracted Terms:\n",
            "* abattoir\n",
            "* aboiteau\n",
            "* acaricide\n",
            "* acre (ac)\n",
            "* acreage\n",
            "* acre-foot\n",
            "* adjuvant\n",
            "* aerial seeding\n",
            "* aeroponics\n",
            "* agrarian system\n",
            "* agrarianism\n",
            "* agribusiness\n",
            "* agricultural aircraft\n",
            "* agricultural cooperative\n",
            "* agricultural cycle\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(terms_list, columns=['Term'])\n",
        "\n",
        "# Define the filename for the CSV file\n",
        "csv_filename = 'agricultural_terms.csv'\n",
        "\n",
        "# Save the DataFrame to CSV. 'index=False' prevents pandas from writing row numbers.\n",
        "df.to_csv(csv_filename, index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "Q26dzP3QInRT"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}